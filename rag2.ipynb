{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheatBot\n",
    "##### The primary goal of this bot is to generate answers based on a piece of data given.\n",
    "\n",
    "## TechStack\n",
    "Gemini LLm <br>\n",
    "Langchain <br>\n",
    "GeminiEmbeddings <br>\n",
    "ChromaDB as a Vector Database<br>\n",
    "Various other python libraries to enhancing the bot<br>\n",
    "<br>\n",
    "#### - Developed by Mihiresh Joshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install all these libraries using pip command\n",
    "# Here I made your work easy (use 'pip3' if on pip3 version): \n",
    "# pip install PyPDF2 langchain python-dotenv pandas chromadb google-api-python-client tqdm\n",
    "\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "from chromadb.api.types import Embeddings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from google.generativeai import GenerationConfig, GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyD5detVlrgZRiQALy7k_L1_QGBHniUIXnc\"  # Change it with your API key\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment. 1. \n",
      "Perform Data Pre-processing on a Dataset \n",
      " \n",
      " \n",
      "Aim: Perform data pre-processing on a dataset \n",
      "Hardware/Software requirements : python  \n",
      "Course Outcomes: Understand the basic concepts of machine learning  \n",
      "Theory  : Data pre-processing is a process of preparing the raw data a nd making it suitable for \n",
      "a machine learning model. It is the first and crucial step whil e creating a machine learning \n",
      "model. When creating a machine learning project, it is not alwa ys a case that we come across \n",
      "the clean and formatted data. And while doing any operation wit h data, it is mandatory to clean \n",
      "it and put in a formatted way. So for this, we use data pre-pro cessing task. \n",
      "A real-world data generally cont ains noises, missing values, an d maybe in an unusable format \n",
      "which cannot be directly used for machine learning models. Data  pre-processing is required \n",
      "tasks for cleaning the data and making it suitable for a machin e learning model which also \n",
      "increases the accuracy and efficiency of a machine learning mod el. It involves below steps: \n",
      " Getting the dataset \n",
      " Importing libraries \n",
      " Importing datasets \n",
      " Finding Missing Data \n",
      " Splitting dataset into training and test set \n",
      " Feature scaling \n",
      "Get the Dataset \n",
      "The collected data for a particular problem in a proper format is known as the dataset . \n",
      "Importing Libraries \n",
      "In order to perform data preprocessing using Python, we need to  import some predefined \n",
      "Python libraries. \n",
      "Importing the Datasets \n",
      "Save the dataset with .csv format and then import the data with  read_csv() function of pandas \n",
      "library. \n",
      "Handling Missing data: C alculating the mean we will calculate the mean of that column o r \n",
      "row which contains any missing value and will put it on the pla ce of missing value. This \n",
      "strategy is useful for the features which have numeric data suc h as age, salary, year, etc. Here, \n",
      "we will use this approach. \n",
      "Splitting the Dataset into the Training set and Test set In machine learning data preprocessing, we divide our dataset i nto a training set and test set. \n",
      "This is one of the crucial steps of data preprocessing as by do ing this, we can enhance the \n",
      "performance of our machine learning model.Suppose, if we have g iven training to our machine \n",
      "learning model by a dataset and we test it by a completely diff erent dataset. Then, it will create \n",
      "difficulties for our model to understand the correlations betwe en the models. \n",
      "If we train our model very well and its training accuracy is al so very high, but we provide a \n",
      "new dataset to it, then it will decrease the performance. So we  always try to make a machine \n",
      "learning model which performs well with the training set and al so with the test dataset. Here, \n",
      "we can define these datasets as: \n",
      " \n",
      "Training Set: A subset of dataset to train the machine learning  model, and we already know \n",
      "the output. \n",
      "Test set: A subset of dataset to test the machine learning mode l, and by using the test set, \n",
      "model predicts the output. \n",
      "Feature Scaling:  Feature scaling is the final step of data preprocessing in mac hine learning. \n",
      "It is a technique to standardize the independent variables of t he dataset in a specific range. In \n",
      "feature scaling, we put our variables in the same range and in the same scale so that no any \n",
      "variable dominate the other variable. \n",
      "we will create the object of StandardScaler  class for independent variables or features. And \n",
      "then we will fit and transform the training dataset. \n",
      "Conclusion : Students are  able to understand the need of pre-processing ste ps and \n",
      "implemented same with python on the available dataset. \n",
      "Viva Questions:  \n",
      " What is Data Preprocessing? \n",
      " What do you mean by feature scaling or data normalization? Expl ain some techniques \n",
      "for feature scaling? \n",
      " What are the missing values? and How do you handle missing valu es? \n",
      " \n",
      "Reference link : \n",
      " https://www.youtube.com/watch?v=NSxEiohAH5o  \n",
      " https://towardsdatascience.com/da ta-preprocessing-in-python-b52 b652e37d5  \n",
      " \n",
      " \n",
      "Experiment No.2  \n",
      "1. Aim:  Implementation of Principal Component Analysis   \n",
      "2. Objectives:  \n",
      "● To understand the basic concepts and importance of pre-processi ng of data \n",
      "analysis \n",
      "● To apply simple PCA reduction technique on actual dataset \n",
      "3. Outcomes : Students will be able to implement PCA reduction techniques. \n",
      "4. Hardware / Software Required: Python 3.10  \n",
      "5. Theory: Principal Component Analysis is an unsupervised learning algori thm that is \n",
      "used for the dimensionality reduction in machine learning. It i s a statistical process that \n",
      "converts the observations of co rrelated features into a set of linearly uncorrelated \n",
      "features with the help of orthogonal transformation. These new transformed features \n",
      "are called the Principal Components. It is one of the popular t ools that is used for \n",
      "exploratory data analysis and predictive modelling. It is a tec hnique to draw strong \n",
      "patterns from the given dataset by reducing the variances. \n",
      "Some real-world applications of PCA are image processing, movie  recommendation \n",
      "system, optimizing the power allocation in various communicatio n channels. It is a \n",
      "feature extraction technique, so it contains the important vari ables and drops the least \n",
      "important variable. \n",
      " The PCA algorithm is based on some mathematical concepts such a s: \n",
      "● Variance and Covariance \n",
      "● Eigenvalues and Eigen factors \n",
      "Some common terms used in PCA algorithm: Dimensionality : It is the number of features or variables present in the give n dataset. \n",
      "More easily, it is the number of columns present in the dataset . \n",
      "Correlation : It signifies that how strongly two variables are related to e ach other. Such \n",
      "as if one changes, the other variable also gets changed. The co rrelation value ranges \n",
      "from -1 to +1. Here, -1 occurs if variables are inversely propo rtional to each other, and \n",
      "+1 indicates that variables are directly proportional to each o ther. \n",
      "Orthogonal : It defines that variables are not correlated to each other, a nd hence the \n",
      "correlation between the pai r of variables is zero. \n",
      "Eigenvectors : If there is a square matrix M, and a non-zero vector v is giv en. Then v \n",
      "will be eigenvector if Av is the scalar multiple of v. Covariance Matrix : A matrix containing the covariance between the pair of variab les \n",
      "is called the Covariance Matrix.  \n",
      "The transformed new features or the output of PCA are the Princ ipal Components. The \n",
      "number of these PCs are either equal to or less than the origin al features present in the \n",
      "dataset. Some properties of these principal components are give n below: \n",
      "The principal component must be the linear combination of the o riginal features. These components are orthogonal, i.e., the correlation between a pair of variables is \n",
      "zero. \n",
      "The importance of each component decreases when going to 1 to n , it means the 1 PC \n",
      "has the most importance, and n PC will have the least importanc e. \n",
      "Applications of Principal Component Analysis: \n",
      "● PCA is mainly used as the dimensionality reduction technique in  various AI \n",
      "applications such as computer v ision, image compression, etc. \n",
      "● It can also be used for finding hidden patterns if data has hig h dimensions. Some \n",
      "fields where PCA is used are Fin ance, data mining, Psychology, etc.  \n",
      "6. Steps to implement PCA in python \n",
      "I. Import  the python  libraries \n",
      "II  Import dataset  \n",
      " \n",
      " \n",
      "III. Apply PCA  \n",
      " \n",
      "● Standardize the dataset prior to PCA. \n",
      "● Import PCA from sklearn.decomposition. \n",
      "● Choose the number of principal components. \n",
      "IV. Check Components \n",
      " \n",
      "The principal components_ provide an array in which the number of rows tells the number \n",
      "of principal components while the number of columns is equal to  the number of features in \n",
      "actual data.  We can easily see that there are three rows as n components was chosen to be \n",
      "3. However, each row has 30 columns as in actual data. \n",
      "V. Plot the components (Visualization)   \n",
      "Plot the principal components for better data visualization.   \n",
      " \n",
      "For three principal components, we need to plot a 3d graph. x[: ,0] signifies the first principal \n",
      "component. Similarly, x[:,1] and x[:,2] represent the second an d the third principal \n",
      "component. \n",
      " \n",
      "VI. Calculate variance ratio \n",
      " \n",
      "7. Conclusion: Implemented PCA and understand the use and its applications. \n",
      " 8. Viva Questions:  \n",
      "● What is a PCA?  \n",
      "● Can you list out the critical assumptions of linear regression?  ● What is the primary difference between R square and adjusted R \n",
      "square? \n",
      "● How Principal Component Analysis  (PCA) is used for Dimensionali ty \n",
      "Reduction?  \n",
      " \n",
      " Experiment. 3 \n",
      " \n",
      " Aim: To implement Linear Regression. \n",
      " \n",
      "Software Required: (Students should write Software required  based on software used for \n",
      "implementing program) e.g., python  \n",
      " \n",
      "Theory:  Linear regression is a basic predictive analytics technique th at uses historical data to \n",
      "predict an output variable. It is popular for predictive modeli ng because it is easily understood. \n",
      "Linear regression models have many real-world applications in a n array of industries such as \n",
      "economics (e.g., predicting growth), business (e.g., predicting  product sales, employee \n",
      "performance), social science (e.g., predicting political leanin gs from gender or race), healthcare \n",
      "(e.g., predicting blood pressure  levels from weight, disease on set from biological factors), and \n",
      "more. \n",
      "The basic idea is that if we can fit a linear regression model to observed data, we can then use \n",
      "the model to predict any future values. There are two kinds of variables in a l inear regression \n",
      "model: \n",
      " The input or predictor variable is the variable(s) that help pr edict the value of the output \n",
      "variable. It is commonly referred to as X. \n",
      " The output variable is the variable that we want to predict. It  is commonly referred to \n",
      "as Y. \n",
      "To estimate Y using linear regression, we assume the equation: \n",
      "Yₑ = α + β X \n",
      "where Yₑ is the estimated or predicted value of Y based on our linear equation. \n",
      "Our goal is to find statistically significant values of the par ameters α and β that minimize the \n",
      "difference between Y and Yₑ. If we are able to determine the op timum values of these two \n",
      "parameters, then we will have the line of best fit that we can use to predict the values of Y, \n",
      "given the value of X. How do we estimate α and β? We can use a method called ordinary least \n",
      "squares. \n",
      "Ordinary Least Squares \n",
      " \n",
      "Green lines show the difference between actual values Y and estimate values Yₑ \n",
      "The objective of the least squares method is to find values of α and β that minimize the sum of \n",
      "the squared difference between Y and Yₑ. We will not go through  the derivation here, but using \n",
      "calculus we can show that the values of the unknown parameters are as follows: \n",
      " \n",
      "where X ̄ is the mean of X values and Ȳ is the mean of Y values. \n",
      "If you are familiar with statistics, you may recognize β as sim ply  \n",
      "Cov(X, Y) / Var(X). \n",
      "Implementation Steps: \n",
      " \n",
      "1. Generate data with nonzero mean and standard deviation (X). Als o, create random \n",
      "data by multiplying a constant and adding residual which is an actual data (Y). \n",
      "2. Calculate the mean of X and Y \n",
      "Mean_X = X1+X2+…X_N/ N \n",
      "Mean_Y = Y1+Y2+…Y_N/ N \n",
      "3. Calculate α and β using  \n",
      " \n",
      " \n",
      "4. Compute the predicted output y \n",
      "Yₑ = α + β X \n",
      "5. Plot regression against actual data \n",
      "  \n",
      "Output Analysis: (Students should write output analysis based on the output. Spe cify each \n",
      "output explicitly with output analysis)  \n",
      " \n",
      " \n",
      "Additional Learning: (Students should write additional learning on their own based o n what \n",
      "additionally they learnt after performing the experiment)  \n",
      " \n",
      " Regression analysis allows you to understand the strength of re lationships between \n",
      "variables. \n",
      " Regression analysis tells you what predictors in a model are st atistically significant and \n",
      "which are not. \n",
      " Regression analysis can give a confidence interval for each reg ression coefficient that \n",
      "it estimates \n",
      " The simple linear regression method tries to find the relations hip between a single \n",
      "independent variable and a corresponding dependent variable. Th e independent \n",
      "variable is the input, and the corresponding dependent variable  is the output. \n",
      " The multiple linear regression method tries to find the relatio nship between two or \n",
      "more independent variables and the corresponding dependent vari able. There's also a \n",
      "special case of multiple linear regression called polynomial re gression. There are other \n",
      "non-linear regression methods used for highly complicated data analysis. \n",
      "Conclusion: (Students should write proper conclusion (in the form of discus sion) on their own) \n",
      " \n",
      " Linear regression is a statistical method that tries to show a relationship between \n",
      "variables. It looks at different data points and plots a trend line. A simple example of \n",
      "linear regression is finding that  the cost of repairing a piece  of machinery increases with \n",
      "time. \n",
      " More precisely, linear regression is used to determine the char acter and strength of the \n",
      "association between a dependent variable and a series of other independent variables. \n",
      "It helps create models to make predictions. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " Experiment. 4 \n",
      " \n",
      "Aim: To implement Logistic Regression. \n",
      " \n",
      "Software required: (Students should write Software required  based on software used for \n",
      "implementing program) \n",
      " Theory: In statistics logistic regression is used to model the probabil ity of a certain class or \n",
      "event. Logistic regression is similar to linear regression beca use both of these involve \n",
      "estimating the values of parameters used in the prediction equa tion based on the given training \n",
      "data. Linear regression predicts the value of some continuous, dependent variable. Whereas \n",
      "logistic regression predicts the probability of an event or cla ss that is dependent on other \n",
      "factors. Thus, the output of logistic regression always lies be tween 0 and 1. Because of this \n",
      "property it is commonly used for classification purpose. \n",
      "Logistic Model \n",
      "Consider a model with features x1, x2, x3 … xn . Let the binary output be denoted by Y, that can \n",
      "take the values 0 or 1. Let p be the probability of Y = 1 , we can denote it as p = P(Y=1) . \n",
      "The mathematical relationship between these variables can be de noted as: \n",
      " \n",
      "Here the term p/(1−p) is known as the odds  and denotes the likelihood of the event taking \n",
      "place. Thus  ln(p/(1−p)) is known as the log odds  and is simply used to map the probability that \n",
      "lies between 0 and 1 to a range between ( −∞,+∞). The terms b0, b1, b2… are parameters (or \n",
      "weights) that we will estimate during training. \n",
      "So we simplify the equation to obtain the value of p:  \n",
      "1. The log term ln on the LHS can be removed by raising the RHS as  a power of e: \n",
      " \n",
      "2. Now we can easily simplify to obtain the value of p : \n",
      " \n",
      "3. This actually turns out to be the equation of the Sigmoid Funct ion which is widely used \n",
      "in other machine learning applications. The Sigmoid Function is  given by: \n",
      " \n",
      " \n",
      "4. Now we will be using the above derived equation to make our pre dictions. Before that \n",
      "we will train our model to obtain the values of our parameters b0, b1, b2… that result \n",
      "in least error. This is where the error or loss function comes in. \n",
      "Loss Function \n",
      "The loss is basically the error in our predicted value. In othe r words, it is a d ifference between \n",
      "our predicted value and the actual value. We will be using the L2 Loss Function to calculate \n",
      "the error. Theoretically you can use any function to calculate the error. This function can be \n",
      "broken down as: \n",
      "1. Let the actual value be yᵢ. Let the value predicted using our m odel be denoted as \n",
      "ȳᵢ. Find the difference between the actual and predicted value.  \n",
      "2. Square this difference. \n",
      "3. Find the sum across all the values in training data. \n",
      " \n",
      "Now that we have the error, we need to update the values of our  parameters to minimize this \n",
      "error. This is where the “learning” actually happens, since our  model is updating itself based \n",
      "on its previous output to obtain a more accurate output in the next step. Hence with each \n",
      "iteration our model becomes mor e and more accurate. We will be using the Gradient Descent \n",
      "Algorithm  to estimate our parameters. Another commonly used algorithm is  the Maximum \n",
      "Likelihood Estimation. \n",
      "The Gradient Descent Algorithm \n",
      "You might know that the partial derivative of a function at its  minimum value is equal to 0. So \n",
      "gradient descent basically uses this concept to estimate the pa rameters or weights of our model \n",
      "by minimizing the loss function. For simplicity, assume that ou r output depends only on a \n",
      "single feature x. So, we can rewrite our equation as: \n",
      "(1) \n",
      "Thus, we need to estimate the values of weights b0 and b1 using  our given training data. \n",
      "1. Initially let b0=0 and b1=0. Let L be the learning rate. The le arning rate controls by \n",
      "how much the values of b0 and b1 are updated at each step in th e learning process. Here \n",
      "let L=0.001. \n",
      "2. Calculate the partial derivative with respect to b0 and b1. The  v a l u e  o f  t h e  p a r t i a l  \n",
      "derivative will tell us how far the loss function is from its m inimum value. It is a \n",
      "measure of how much our weights need to be updated to attain mi nimum or ideally 0 \n",
      "error. In case you have more than one feature, you need to calc ulate the partial \n",
      "derivative for each weight b0, b1 … bn where n is the number of  features. \n",
      "(2) \n",
      "3. Next we update the values of b0 and b1: \n",
      "(3) \n",
      "4. We repeat this process until our loss function is a very small value or ideally reaches 0 \n",
      "(meaning no errors and 100% accuracy). The number of times we r epeat this learning \n",
      "process is known as iterations or epochs.  \n",
      "Steps: \n",
      "1. Read and visualize the database \n",
      "2. Divide the data to training set and test set (80:20) \n",
      "3. Perform data normalization 4. Compute the method to make predictions using equation (1) \n",
      "5. Evaluate the method to train the model using equations (2) a nd (3). Obtain b0 \n",
      "and b1 \n",
      "6. Train the model using training data \n",
      "7. Make the predictions  8. Calculate accuracy \n",
      " \n",
      "Output Analysis: (Students should write output analysis based on the output. Spe cify each \n",
      "output explicitly with output analysis)  \n",
      " \n",
      "Additional Learning: (Students should write additional learning on their own based o n \n",
      "what additionally)  \n",
      " Linear regression predicts the continuous dependent variable fo r a given set of \n",
      "independent variables, logistic regression predicts the categor ical dependent \n",
      "variable.  \n",
      " Linear regression is used to sol ve regression problems, logisti c regression is \n",
      "used to solve classification problems.  \n",
      "  \n",
      "Conclusion: (Students should write proper conclusion (in the form of discus sion) on their own)  \n",
      " \n",
      " Logistic regression can solve re gression problems, but it's mai nly used for classification \n",
      "problems. Its output can only be 0 or 1. It's valuable in situa tions where we need to \n",
      "determine the probabilities between two classes or, in other wo rds, calculate the \n",
      "likelihood of an event.  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Machine Learning Lab \n",
      "Experiment No. : 5 \n",
      "McCulloch-Pitt’s Model \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      "Experiment No.5   1. Aim: Implement a basic McCulloch-Pitt’s Model for any logical functi on. \n",
      "2. Objectives: \n",
      "● Understand the fundamentals of Artificial Neural Network. \n",
      "● Explore various machine learning tools and techniques. \n",
      "Outcomes:  \n",
      "● Students will be able to use neural network techniques for lear ning. \n",
      " \n",
      "3. Software (tools/packages) Required : Python  \n",
      " \n",
      "4.  Theory:   \n",
      " \n",
      "Basic Theory of NN- \n",
      " Neural networks are parallel computing devices, which is basica lly an attempt to make \n",
      "a computer model of the brain. The main objective is to develop  a system to perform \n",
      "various computational tasks faster than the traditional systems . These tasks include \n",
      "pattern recognition and classification, approximation, optimiza tion, and data clustering. \n",
      " \n",
      "                                Fig: Basic structure of Neural Net \n",
      "Basic operation of NN- \n",
      "  X1 and X2 – input neurons.  \n",
      " Y- output neuron  \n",
      " Weighted interconnection links- W1 and W2.  \n",
      " Net input calculation is :  y1 = -x1w1 + x2w2 \n",
      " Output is : y= f(Y in) \n",
      " The function to be applied over the net input is called activa tion function. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "McCulloch-Pitt’s Model- \n",
      "   \n",
      "Step1: Provide Training data (Truth table of any logical operation) \n",
      "Step2: Assume weights  \n",
      "Step3: Draw NN architecture  \n",
      "Step4: Calculate net input \n",
      "            Y in = X 1W1 + X2W2 +...................+ XnWn \n",
      "Step5: Choose threshold to apply activation function \n",
      "Step6: Calculate output using activation function \n",
      "           F(y in) = { 1, if Y in >= ɵ \n",
      " \n",
      "                           0, if Y in < ɵ } \n",
      "  \n",
      "5. Output Analysis: (Students should write output analysis based o n the output. \n",
      "Specify each output explicitly with output analysis) \n",
      " \n",
      "6. Additional Learning: (Students should write additional learning  on their own \n",
      "based on what additionally they learnt after performing the exp eriment) \n",
      " \n",
      "7.  Conclusion:  (Students should write proper conclusion (in the form of discus sion)on \n",
      "their own)  \n",
      "Most of the work is carried on the basis of MCP model for obser ving the nature of logic \n",
      "gates like OR, AND, NOT, NAND, NOR, XOR with variable threshold  conditions and \n",
      "for variable weights.  \n",
      " \n",
      "8. Viva Questions: \n",
      "● What basic components of ANN? \n",
      "● What activation function is used in M-P Model ? \n",
      " \n",
      "References:  \n",
      " \n",
      "https://pabloinsente.github.io/the-mcculloch-pitts-artificial-n euron-model  \n",
      " \n",
      "https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5d d1 \n",
      " \n",
      "   \n",
      "Machine Learning Lab \n",
      "Experiment No. : 6 \n",
      "Naive Bayes classification  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "  \n",
      "Experiment No.6 \n",
      "9. Aim: Implement Naive Bayes classification algorithm.  \n",
      " \n",
      "10. Objectives: \n",
      "● Analyse the data, identify the problem and choose relevant algo rithms to apply. \n",
      "● Identify the classification application in machine learning. \n",
      "Outcomes:  \n",
      "● Students will be able to understand and implement classificatio n algorithms. \n",
      " \n",
      "11. Software Required : R Studio /Python \n",
      " \n",
      "12.  Theory:   \n",
      "        Naïve Bayes algorithm is a supervised learning algorith m, which is \n",
      "based on Bayes theorem and used for solving classification prob lems. Naïve Bayes \n",
      "Classifier is one of the simple and most effective Classificati on algorithms which helps in \n",
      "building the fast machine learning models that can make quick p redictions. It is a \n",
      "probabilistic classifier, which means it predicts on the basis of the probability of an object. \n",
      "Some popular examples of Naïve Bayes Algorithm are spam filtrat ion, Sentimental \n",
      "analysis, and classifying articles.  \n",
      "Bayes' theorem is used to determine the probability of a hypoth esis with prior knowledge. \n",
      "It  depends on the conditional probability. The formula for Bay es' theorem is given as:  \n",
      "                                                 \n",
      "P(A|B) is Posterior probability: Probability of hypothesis A on  the observed event B \n",
      "P(B|A) is Likelihood probability: Probability of the evidence g iven that the probability of \n",
      "a hypothesis is true \n",
      "P(A) is Prior Probability: Proba bility of hypothesis before obs erving the evidence \n",
      "P(B) is Marginal Probability: Probability of Evidence.  \n",
      "    \n",
      " \n",
      "e.g. \n",
      " \n",
      " \n",
      "            \n",
      "13. Output Analysis: (Students should write output analysis based o n the output. \n",
      "Specify each output explicitly with output analysis) \n",
      " \n",
      " \n",
      "14. Additional Learning: (Students should write additional learning  on their own \n",
      "based on what additionally they learnt after performing the exp eriment) \n",
      " \n",
      "15.  Conclusion:  (Students should write proper conclusion (in the form of discus sion)on \n",
      "their own)  \n",
      "Classification is a supervised  learning problem. Through this e xperiment the need \n",
      "for classification algorithm was recognized and understood, and  we implemented \n",
      "a navie bayes classification algorithm which uses a probabilist ic approach.  \n",
      " \n",
      "16. Viva Questions: \n",
      "● What is supervised learning? \n",
      "● What are various classification algorithms? \n",
      "● What is prior probability of a class? \n",
      "References: \n",
      " \n",
      "https://www.datacamp.com/tutori al/naive-bayes-scikit-learn  \n",
      "https://www.javatpoint.com/machine‐learning‐naive‐bayes‐classif ier  \n",
      "  \n",
      "Machine Learning \n",
      " Experiment No.: 7 \n",
      "Implement Support Vector Machine for \n",
      "binary classification \n",
      "     \n",
      " Experiment No. 7 \n",
      "1. Aim:  To Implement Support Vector Machine for binary classification. \n",
      " \n",
      " \n",
      "2. Objectives: From this experiment, the student will be able to  \n",
      "● Familiar with support vector machine. \n",
      "● Understand and implement classification algorithms in machine l earning. \n",
      "● Analyse the large data using SVM.  \n",
      " \n",
      "3. Outcomes: The learner will be able to  \n",
      " \n",
      "● Understand and design Support Vect or Machines for classificatio n of linear data.  \n",
      " \n",
      "4. Software Required :  Weka/ Java / Python/ Matlab / R-Tool etc. \n",
      "5. Theory:   \n",
      "A Support Vector Machine (SVM) is a supervised machine learning  algorithm that can be \n",
      "employed for both classification and regression purposes. SVMs are more commonly used \n",
      "in classification problems. SVMs are based on the idea of findi ng a hyper plane that best \n",
      "divides a dataset into two classes, as shown in the image below . \n",
      ".  \n",
      "Support Vectors \n",
      "  \n",
      "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if \n",
      "removed, would alter the position of the dividing hyperplane. B ecause of this, they can be \n",
      "considered the critical elements of a data set. \n",
      "What is a hyperplane? \n",
      "  \n",
      "As a simple example, for a classification task with only two fe atures (like the image above), \n",
      "you can think of a hyperplane as a line that linearly separates  and classifies a set of data. \n",
      "Intuitively, the further from the hyperplane our data points li e, the more confident we are that \n",
      "they have been correctly classified. We therefore want our data  points to be as far away from \n",
      "the hyperplane as possible, whi le still being on the correct si de of it. \n",
      "So when new testing data is added, whatever side of the hyperpl ane it lands will decide the \n",
      "class that we assign to it. \n",
      "How do we find the right hyperplane? \n",
      "  \n",
      "Or, in other words, how do we best segregate the two classes wi thin the data? \n",
      "The distance between the hyperplane and the nearest data point from either set is known as the \n",
      "margin. The goal is to choose a hyperplane with the greatest po ssible margin between the \n",
      "hyperplane and any point within the training set, giving a grea ter chance of new data being \n",
      "classified correctly. \n",
      " \n",
      "Pros & Cons of Support Vector Machines \n",
      " Pros  \n",
      "● Accuracy \n",
      "● Works well on smaller cleaner datasets \n",
      "● It can be more efficient because it uses a subset of training p oints \n",
      "Cons  \n",
      "● Isn’t suited to larger datasets as the training time with SVMs can be high \n",
      "● Less effective on noisier datas ets with overlapping classes \n",
      "6. Procedure/Program: \n",
      "The operation of the SVM algor ithm is based on finding the hype rplane that gives the \n",
      "largest minimum distance to the training examples. Twice, this distance receives the \n",
      "important name of margin  within SVM’s theory. Therefore, the optimal separating \n",
      "hyperplane maximizes  the margin of the training data. \n",
      " \n",
      "How is the optimal hyperplane computed? \n",
      " \n",
      "Step 1: define formally a hyperplane: \n",
      " \n",
      "where is known as the weight vector  and as the bias. \n",
      "Step 2: The optimal hyperplane can be represented in an infinit e number of different ways by \n",
      "scaling of  and . As a matter of convention, among all the possi ble representations of the \n",
      "hyperplane, the one chosen is \n",
      " \n",
      "where  symbolizes the training examples closest to the hyperpla ne. In general, the training \n",
      "examples that are closest to the hyperplane are called support vectors . This representation is \n",
      "known as the canonical hyperplane . \n",
      "Step 3 : Now, use the result of geometry that gives the distanc e between a point  and a \n",
      "hyperplane : \n",
      " \n",
      "Step 5 : In particular, for the canonical hyperplane, the numer ator is equal to one and the \n",
      "distance to the support vectors is \n",
      " \n",
      "Step 6: Recall that the margin introduced in the previous secti on, here denoted as , is twice \n",
      "the distance to the closest examples: \n",
      " \n",
      "Step 7: Finally, the problem of maximizing is equivalent to the  problem of minimizing a \n",
      "function subject to some constraints. The constraints model the  requirement for the \n",
      "hyperplane to classify correctly all the training examples . Fo rmally, \n",
      " \n",
      "where represents each of the labels of the training examples. \n",
      "This is a problem of Lagrangian optimization that can be solved  using Lagrange multipliers to \n",
      "obtain the weight vector and the bias of the optimal hyperplane . \n",
      " \n",
      "7. Steps Using Weka: \n",
      "1. Select Explorer \n",
      "2. Click: Open File \n",
      "3. Choose database.arff  (from DATA Folder) \n",
      "4. Click: Open \n",
      "5. Click: Classify \n",
      "6. Under Classifier: Choose functions>SMOreg \n",
      "7. Click: Close \n",
      "8. Confirm cross-validation Folds = 10 \n",
      "9. Right-click on the text box to to the right of the choose butto n \n",
      "10. Under Kernel choose Polykernel \n",
      "11. Click Close \n",
      "12. Set c = 1 \n",
      "13. Set Filter Type to Normalize training data \n",
      "14. Click OK \n",
      "15. Click Start \n",
      " \n",
      " \n",
      "8. Results: \n",
      " \n",
      "The operation of the SVM algorithm is based on finding the hype rplane that gives the largest \n",
      "minimum distance to the training examples. Twice, this distance  receives the important name \n",
      "of margin  within SVM’s theory. Therefore, the optimal separating hyperpl ane maximizes  the \n",
      "margin of the training data. \n",
      " \n",
      "9. Conclusion: \n",
      "The different classification algorithms of data mining were stu died and one among them named \n",
      "support vector machine algorithm was implemented. The need for classification algorithm was \n",
      "recognized and understood.  \n",
      "10. Viva Questions:  \n",
      "● What is hyperplane? \n",
      "● What are the different  kernel functions? \n",
      "● What is the role of ma ximum margin in SVM? \n",
      "11. References: \n",
      "●  Tom M.Mitchell “Machine Learning” McGraw Hill \n",
      "  \n",
      " \n",
      "Machine Learning \n",
      "Experiment No. : 8 \n",
      "Implement K-means clustering \n",
      "algorithm. \n",
      " \n",
      "    \n",
      " \n",
      " Experiment No. 8 \n",
      "1. Aim: Implement K-means clustering algorithm. \n",
      " \n",
      "2. Objectives: From this experiment, the student will be able to  \n",
      "● Analyse the data, identify the problem and choose relevant algo rithm to apply \n",
      "● Understand and implement classical clustering algorithms in dat a mining \n",
      "● Identify the application of clustering algorithm in data mining  \n",
      "3. Outcomes: The learner will be able to  \n",
      "● Assess the strength and weaknesses of algorithms \n",
      "● Identify, formulate and solve engineering problems \n",
      "● Analyse the local and global impact of data mining on individua ls, organizations \n",
      "and society  \n",
      "4. Software Required:  Python/R/Java etc. \n",
      "5. Theory:  \n",
      "Clustering is dividing data points into homogeneous classes or clusters: \n",
      "● Points in the same group are as similar as possible \n",
      "● Points in different group are as dissimilar as possible \n",
      "When a collection of objects is given, we put objects into grou p based on similarity. \n",
      "Clustering Algorithms: \n",
      "A Clustering Algorithm tries to analyse natural groups of data on the basis of some \n",
      "similarity. It locates the centr oid of the group of data points . To carry out effective \n",
      "clustering, the algorithm evaluates the distance between each p oint from the centroid of \n",
      "the cluster. The goal of clustering is to determine the intrins ic grouping in a set of \n",
      "unlabelled dataTheory:   \n",
      " \n",
      "K-means Clustering \n",
      "K-means (Macqueen, 1967) is one o f the simplest unsupervised le arning algorithms \n",
      "that solve the well-known clustering problem. K-means clusterin g is a method of \n",
      "vector quantization, originall y from signal processing, that is  popular for cluster \n",
      "analysis in data mining. \n",
      " \n",
      " \n",
      "6. Procedure: \n",
      "Input: \n",
      "             K: the number of clusters         D: a data set co ntaining n objects. \n",
      " \n",
      " Output: A set of k clusters.  \n",
      "i. Arbitrarily choose K objects from D as the initial cluster cent ers \n",
      "ii. Partition of objects into k non-empty subsets \n",
      "iii. Identifying the cluster centroids (mean point) of the current p artition. \n",
      "iv. Assigning each point to a specific cluster \n",
      "v. Compute the distances from each point and allot points to the c luster where \n",
      "the distance from the centroid is minimum. \n",
      "vi. After re-allotting the points, find the centroid of the new clu ster formed. \n",
      "7. Conclusion: \n",
      "The different clustering algorithms of data mining were studied  and one among them \n",
      "named k-means clustering algorithm was implemented using …..Lan guage. The need \n",
      "for clustering algorithm was recognized and understood. \n",
      "8. Viva Questions:  \n",
      "● What are different clu stering techniques? \n",
      "● What is difference between K-means and K-medoids? \n",
      "● What is dendogram? \n",
      "9.  References: \n",
      " \n",
      "● Han, Kamber, \"Data Mining Concepts and Techniques\", Morgan Kauf mann 3rd \n",
      "Edition \n",
      "● M.H. Dunham, \"Data Mining Introductory and Advanced Topics\", Pe arson Education \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Machine Learning Lab \n",
      "Experiment No. : 9 \n",
      "Agglomerative Clustering  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " Experiment No.9  \n",
      "1. Aim: Implementing Agglomerative Clustering.  \n",
      "2. Objectives: \n",
      " Understand the Agglomerative Clustering. \n",
      " Explore various machine learning tools and techniques. \n",
      "Outcomes:  \n",
      " Students will be able to use Agglomerative Clustering technique s for \n",
      "classification. \n",
      " \n",
      "3. Software (tools/packages) Required : Python  \n",
      " \n",
      "4.  Theory:   \n",
      " \n",
      "Agglomerative Clustering is a type of hierarchical clustering a lgorithm. It is an \n",
      "unsupervised machine learning te chnique that divides the popula tion into several \n",
      "clusters such that data points in the same cluster are more sim ilar and data points in \n",
      "different clusters are dissimilar. \n",
      "1. Points in the same cluster are closer to each other. \n",
      "2. Points in the different clusters are far apart. \n",
      "Agglomerative Clustering is a bottom-up approach, initially, ea ch data point is a cluster \n",
      "of its own, further pairs of clusters are merged as one moves u p the hierarchy. \n",
      "Steps of Agglomerative Clustering: \n",
      "1. Initially, all the data-points are a cluster of its own. \n",
      "2. Take two nearest clusters and join them to form one single clus ter. \n",
      "3. Proceed recursively step 2 until you obtain the desired number of clusters. \n",
      "o obtain the desired number of clusters, the number of clusters  needs to be reduced from \n",
      "initially being n cluster (n equals the total number of data-po ints). Two clusters are \n",
      "combined by computing the similarity between them. \n",
      "There are some methods which are used to calculate the similari ty between two clusters: \n",
      " \n",
      "1. Distance between two closest points in two clusters. \n",
      "2. Distance between two farthest points in two clusters. \n",
      "3. The average distance between all points in the two clusters. \n",
      "4. Distance between centroids of two clusters. \n",
      "5. There are several pros and cons of choosing any of the above si milarity metrics.  \n",
      "5. Output Analysis: (Students should write output analysis based o n the output. \n",
      "Specify each output explicitly with output analysis) \n",
      " \n",
      "6. Additional Learning: (Students should write additional learning  on their own \n",
      "based on what additionally they learnt after performing the exp eriment) \n",
      " \n",
      "7.  Conclusion \n",
      "8. Viva Questions: \n",
      " What is mean Agglomerative Clustering.? \n",
      " What Computing Distance Matrix are used in Agglomerative Cluste ring? \n",
      " \n",
      "References:  \n",
      "1. https://towardsdatascience.com/a gglomerative-clustering-and-den drograms-\n",
      "explained-29fc12b85f23 . \n",
      "2. https://www.javatpoint.com/hierarchical-clustering-in-machine-l earning \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Machine Learning Lab \n",
      "Experiment No.: 10 \n",
      "Case study of reinforcement learning \n",
      "Elevator Dispatching \n",
      " \n",
      " \n",
      "      \n",
      "  \n",
      "Waiting for an elevator is a sit uation with which we are all fa miliar. We press a \n",
      "button and then wait for an eleva tor to arrive traveling in the  right direction. We \n",
      "may have to wait a long time if there are too many passengers o r not enough \n",
      "elevators. Just how long we wait  depends on the dispatching str ategy the elevators \n",
      "use to decide where to go. For e xample, if passengers on severa l floors have \n",
      "requested pickups, which should be served first? If there are n o pickup requests, \n",
      "how should the elevators distribute themselves to await the nex t request? Elevator \n",
      "dispatching is a good example of  a stochastic optimal control p roblem of economic \n",
      "importance that is too large to s olve by classical techniques s uch as dynamic \n",
      "programming. \n",
      "Crites and Barto (1996; Crites, 1996) studied the application o f reinforcement \n",
      "learning techniques to the four -elevator, ten-floor system show n in Figure  11.8 . \n",
      "Along the right-hand side are p ickup requests and an indication  of how long each \n",
      "has been waiting. Each elevator has a position,  direction, and speed, plus a set of \n",
      "buttons to indicate where passenge rs want to get off. Roughly q uantizing the \n",
      "continuous variables, Crites and Barto estimated that the syste m has \n",
      "over  states. This large state set rules out classical dynamic programming \n",
      "methods such as value iteration. E ven if one state could be bac ked up every \n",
      "microsecond it would still require over 1000 years to complete just one sweep \n",
      "through the state space. \n",
      " \n",
      "Figure 11.8:  Four elevators in a ten‐story building.  \n",
      " \n",
      "In practice, modern elevator disp atchers are designed heuristic ally and evaluated \n",
      "on simulated buildings. The simu lators are quite sophisticated and detailed. The \n",
      "physics of each elevator car is modeled in continuous time with  continuous state \n",
      "variables. Passenger arrivals are  modeled as discrete, stochast ic events, with arrival \n",
      "rates varying frequently over th e course of a simulated day. No t surprisingly, the \n",
      "times of greatest traffic and greatest challenge to the dispatc hing algorithm are the \n",
      "morning and evening rush hours. Dispatchers are generally desig ned primarily for \n",
      "these difficult periods. \n",
      "The performance of elevator disp atchers is measured in several different ways, all \n",
      "with respect to an average passenger entering the system. The a verage waiting \n",
      "time is how long the passenger waits  before getting on an elevator,  and the \n",
      "average system time  is how long the passenger waits before being dropped off at \n",
      "the destination floor. Another frequently encountered statistic  is the percentage of \n",
      "passengers whose waiting time exceeds 60 seconds. The objective  that Crites and \n",
      "Barto focused on is the average squared waiting time . This objective is commonly \n",
      "used because it tends to keep the waiting times low while also encouraging fairness \n",
      "in serving all the passengers. \n",
      "Crites and Barto applied a versio n of one-step Q-learning augme nted in several \n",
      "ways to take advantage of speci al features of the problem. The most important of \n",
      "these concerned the formulation of the actions. First, each ele vator made its own \n",
      "decisions independently of the ot hers. Second, a number of cons traints were placed \n",
      "on the decisions. An elevator carrying passengers could not pas s by a floor if any \n",
      "of its passengers wanted to get off there, nor could it reverse  direction until all of \n",
      "its passengers wanting to go in its current direction had reach ed their floors. In \n",
      "addition, a car was not allowed to stop at a floor unless someo ne wanted to get on \n",
      "or off there, and it could not sto p to pick up passengers at a floor if another \n",
      "elevator was already stopped there. Finally, given a choice bet ween moving up or \n",
      "down, the elevator was constrained always to move up (otherwise  evening rush \n",
      "hour traffic would tend to push all the elevators down to the l obby). These last \n",
      "three constraints were explicitl y included to provide some prio r knowledge and \n",
      "make the problem easier. The net  result of all these constraint s was that each \n",
      "elevator had to make few and simple decisions. The only decisio n that had to be \n",
      "made was whether or not to stop at a floor that was being appro ached and that had \n",
      "passengers waiting to be picked  up. At all other times, no choi ces needed to be \n",
      "made. \n",
      "That each elevator made choice s only infrequently permitted a s econd \n",
      "simplification of the problem. A s far as the learning agent was  concerned, the \n",
      "system made discrete jumps from one time at which it had to mak e a decision to \n",
      "the next. When a continuous-time decision problem is treated as  a discrete-time \n",
      "system in this way it is known as a semi-Markov  decision process. To a large \n",
      "extent, such processes can be treated just like any other Marko v decision process \n",
      "by taking the reward on each discr ete transition as the integra l of the reward over \n",
      "the corresponding continuous-tim e interval. The notion of retur n generalizes naturally from a discounted sum o f future rewards to a discount ed integral  of \n",
      "future rewards: \n",
      " \n",
      " where  on the left is the usual immediate reward in discrete ti me and  on the \n",
      "right is the instantaneous reward at continuous time . In the e levator problem \n",
      "the continuous-time reward is th e negative of the sum of the sq uared waiting times \n",
      "of all waiting passengers. The p arameter  plays a role similar to that of the \n",
      "discount-rate parameter .\n",
      " \n",
      "The basic idea of the extensio n of Q-learning to semi-Markov de cision problems \n",
      "can now be explained. Suppose the system is in state  and takes  action  at time , \n",
      "and then the next decision is required at time  in state . Afte r this discrete-event \n",
      "transition, the semi-Markov Q-learning backup for a tabular act ion-value \n",
      "function, , would be: \n",
      " \n",
      "Note how  acts as a variable di scount factor that depends on th e amount of \n",
      "time between events. This method is due to Bradtke and Duff (19 95). \n",
      "One complication is that the reward as defined--the negative su m of the squared \n",
      "waiting times--is not something that would normally be known wh ile an actual \n",
      "elevator was running. This is because in a real elevator system  one does not know \n",
      "how many people are waiting at a floor, only how long it has be en since the button \n",
      "requesting a pickup on that floor  was pressed. Of course this i nformation is known \n",
      "in a simulator, and Crites and Barto used it to obtain their be st results. They also \n",
      "experimented with another techni que that used only information that would be \n",
      "known in an on-line learning situation with a real set of eleva tors. In this case one \n",
      "can use how long since each button has been pushed together wit h an estimate of \n",
      "the arrival rate to compute an expected  summed squared waiting time for each \n",
      "floor. Using this in the reward  measure proved nearly as effect ive as using the \n",
      "actual summed squared waiting time. \n",
      "For function approximation, a non linear neural network trained by \n",
      "backpropagation was used to repre sent the action-value function . Crites and Barto \n",
      "experimented with a wide variety of ways of representing states  to the network. \n",
      "After much exploration, their bes t results were obtained using networks with 47 \n",
      "input units, 20 hidden units, an d two output units, one for eac h action. The way the \n",
      "state was encoded by the input units was found to be critical t o the effectiveness of \n",
      "the learning. The 47 input  units were as follows: \n",
      " 18 units: Two units encoded inf ormation about each of the nine hall buttons \n",
      "for down pickup requests. A real-v alued unit encoded the elapse d time if the \n",
      "button had been pushed, and a binary unit was on if the button had not been \n",
      "pushed. \n",
      " 16 units: A unit for each possibl e location and direction for t he car whose \n",
      "decision was required. Exactly o ne of these units was on at any  given time. \n",
      " 10 units: The location of the oth er elevators superimposed over  the 10 \n",
      "floors. Each elevator had a \"foot print'' that depended on its d irection and \n",
      "speed. For example, a stopped el evator caused activation only o n the unit \n",
      "corresponding to its c urrent floor, but a moving elevator cause d activation \n",
      "on several units corresponding to  the floors it was approaching , with the \n",
      "highest activations on the closes t floors. No information was p rovided about \n",
      "which one of the other cars wa s at a particular location. \n",
      " 1 unit: This unit was on if the e levator whose de cision was req uired was at \n",
      "the highest floor with  a passenger waiting. \n",
      " 1 unit: This unit was on if the e levator whose de cision was req uired was at \n",
      "the floor with the passenger who had been waiting for the longe st amount of \n",
      "time. \n",
      " 1 unit: Bias unit was always on. \n",
      "Two architectures were used. In RL1, each elevator was given it s own action-value \n",
      "function and its own neural netwo rk. In RL2, there was only one  network and one \n",
      "action-value function, with the experiences of all four elevato rs contributing to \n",
      "learning in the one network. In both cases, each elevator made its decisions \n",
      "independently of the other elevat ors, but shared a single rewar d signal with them. \n",
      "This introduced additional stocha sticity as far as each elevato r was concerned \n",
      "because its reward depended in part on the actions of the other  elevators, which it \n",
      "could not control. In the archit ecture in which each elevator h ad its own action-\n",
      "value function, it was possible fo r different elevators to lear n different specialized \n",
      "strategies (although in fact the y tended to learn the same stra tegy). On the other \n",
      "hand, the architecture with a c ommon action-value function coul d learn faster \n",
      "because it learned simultaneously  from the experiences of all e levators. Training \n",
      "time was an issue here, even tho ugh the system was trained in s imulation. The \n",
      "reinforcement learning methods w ere trained for about four days  of computer time \n",
      "on a 100 mips processor (corre sponding to about 60,000 hours of  simulated time). \n",
      "While this is a considerable amo unt of computation, it is negli gible compared with \n",
      "what would be required by any co nventional dynamic programming algorithm. \n",
      "The networks were trained by simulating a great many evening ru sh hours while \n",
      "making dispatching decisions using the developing, learned acti on-value functions. Crites and Barto used the Gibbs  softmax procedure to select act ions as described in \n",
      "Section 2.3, reducing the \"tempe rature\" gradually over training . A temperature of \n",
      "zero was used during test runs on which the performance of the learned dispatchers \n",
      "was assessed. \n",
      " \n",
      "Figure 2:  Comparison of elevator dispatchers. The SECTOR dispatcher is similar \n",
      "to what is used in many actual eleva tor systems. The RL1 and RL2 dispatchers \n",
      "were constructed through reinforcement learning.  \n",
      " \n",
      "Figure  2 shows the performance of several dispatchers during a simulate d evening rush hour, \n",
      "what researchers call down-peak  traffic. The dispatchers include methods similar to those \n",
      "commonly used in the industry, a variety of heuristic methods, sophisticated research \n",
      "algorithms that repeatedly run complex optimization algorithms on-line (Bao et al., 1994), and \n",
      "dispatchers learned by using the two reinforcement learning arc hitectures. By all of the \n",
      "performance measures, the reinforc ement learning dispatchers co mpare favorably with the \n",
      "others. Although the optimal policy for this problem is unknown , and the state of the art is \n",
      "difficult to pin down because details of commercial dispatching  strategies are proprietary, \n",
      "these learned dispatchers appeared to perform very well.  \n",
      " \n",
      "References:  \n",
      "1. http://incompleteideas.net/book/ebook/node107.html \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A very basic function to extract text from pdf\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    extracted_text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        for page_number in range(num_pages):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            extracted_text += page.extract_text()\n",
    "    print(extracted_text)\n",
    "    return extracted_text\n",
    "\n",
    "pdf_path = \"/Users/mihiresh/Downloads/ml_lab.pdf\"  # Don't forget to give your address here \n",
    "text = extract_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text.replace(\"Module 1– Introduction to Social Network\",\" \")\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important step of chunking your textual data. Here you could use complete creativity while fine-tuning\n",
    "# My suggestion try performing content-based chunking\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,    #for next time let's try to keep the chunk size little bigger\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables. It looks at different data points and plots a trend line. A simple example of \n",
      "linear regression is finding that  the cost of repairing a piece  of machinery increases with \n",
      "time. \n",
      " More precisely, linear regression is used to determine the char acter and strength of the \n",
      "association between a dependent variable and a series of other independent variables. \n",
      "It helps create models to make predictions. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " Experiment. 4 \n",
      " \n",
      "Aim: To implement Logistic Regression. \n",
      " \n",
      "Software required: (Students should write Software required  based on software used for \n",
      "implementing program) \n",
      " Theory: In statistics logistic regression is used to model the probabil ity of a certain class or \n",
      "event. Logistic regression is similar to linear regression beca use both of these involve \n",
      "estimating the values of parameters used in the prediction equa tion based on the given training\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([text])\n",
    "print(texts[14].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Experiment. 1. \\nPerform Data Pre-processing on a Dataset \\n \\n \\nAim: Perform data pre-processing on a dataset \\nHardware/Software requirements : python  \\nCourse Outcomes: Understand the basic concepts of machine learning  \\nTheory  : Data pre-processing is a process of preparing the raw data a nd making it suitable for \\na machine learning model. It is the first and crucial step whil e creating a machine learning \\nmodel. When creating a machine learning project, it is not alwa ys a case that we come across \\nthe clean and formatted data. And while doing any operation wit h data, it is mandatory to clean \\nit and put in a formatted way. So for this, we use data pre-pro cessing task. \\nA real-world data generally cont ains noises, missing values, an d maybe in an unusable format \\nwhich cannot be directly used for machine learning models. Data  pre-processing is required \\ntasks for cleaning the data and making it suitable for a machin e learning model which also', 'tasks for cleaning the data and making it suitable for a machin e learning model which also \\nincreases the accuracy and efficiency of a machine learning mod el. It involves below steps: \\n\\uf0b7 Getting the dataset \\n\\uf0b7 Importing libraries \\n\\uf0b7 Importing datasets \\n\\uf0b7 Finding Missing Data \\n\\uf0b7 Splitting dataset into training and test set \\n\\uf0b7 Feature scaling \\nGet the Dataset \\nThe collected data for a particular problem in a proper format is known as the dataset . \\nImporting Libraries \\nIn order to perform data preprocessing using Python, we need to  import some predefined \\nPython libraries. \\nImporting the Datasets \\nSave the dataset with .csv format and then import the data with  read_csv() function of pandas \\nlibrary. \\nHandling Missing data: C alculating the mean we will calculate the mean of that column o r \\nrow which contains any missing value and will put it on the pla ce of missing value. This \\nstrategy is useful for the features which have numeric data suc h as age, salary, year, etc. Here,']\n"
     ]
    }
   ],
   "source": [
    "# Combinig chunks of your textual data to form a List. \n",
    "\n",
    "docs = []\n",
    "\n",
    "for chunk in texts:\n",
    "    docs.append(chunk.page_content)\n",
    "\n",
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        model = 'models/embedding-001'\n",
    "        title = 'API'\n",
    "        return genai.embed_content(\n",
    "            model=model,\n",
    "            content=input,\n",
    "            task_type=\"retrieval_document\",\n",
    "            title=title)[\"embedding\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define and create you Vector Databse of chromadb in Database folders\n",
    "\n",
    "def create_chroma_db(docs,name):\n",
    "    chroma_client = chromadb.PersistentClient(path=\"/Users/mihiresh/Mihiresh/Work/Cheatbot/dsa-java\") #Don't forget to change path\n",
    "    db = chroma_client.get_or_create_collection(\n",
    "        name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    \n",
    "    initial_size = db.count()\n",
    "    for i, d in tqdm(enumerate(docs), total=len(docs), desc=\"/Users/mihiresh/Mihiresh/Work/Cheatbot/dsa-java\"):\n",
    "        db.add(\n",
    "            documents=d,\n",
    "            ids=str(i + initial_size)\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return db\n",
    "\n",
    "\n",
    "def get_chroma_db(name):\n",
    "    chroma_client = chromadb.PersistentClient(path=\"/Users/mihiresh/Mihiresh/Work/Cheatbot/dsa-java\") # Here as well \n",
    "    return chroma_client.get_collection(name=name, function=EmbeddingFunction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihiresh/Mihiresh/Work/Cheatbot/dsa-java: 100%|██████████| 53/53 [00:54<00:00,  1.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating your Vector Database. [Don't unneccessarily run this code]\n",
    "\n",
    "db = create_chroma_db(docs, \"sme_db\")\n",
    "db.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>metadatas</th>\n",
       "      <th>documents</th>\n",
       "      <th>uris</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.022032495588064194, -0.07901791483163834, 0...</td>\n",
       "      <td>None</td>\n",
       "      <td>Experiment. 1. \\nPerform Data Pre-processing o...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.027059582993388176, -0.06932812184095383, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>tasks for cleaning the data and making it suit...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[-0.012819928117096424, -0.0496579147875309, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>economics (e.g., predicting growth), business ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>[-0.012207344, -0.028850332, -0.054086257, -0....</td>\n",
       "      <td>None</td>\n",
       "      <td>For function approximation, a non linear neura...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>[-0.008514339, -0.042759273, -0.043888994, -0....</td>\n",
       "      <td>None</td>\n",
       "      <td>decision was required. Exactly o ne of these u...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>95</td>\n",
       "      <td>[-0.0011812966549769044, -0.052746474742889404...</td>\n",
       "      <td>None</td>\n",
       "      <td>average system time  is how long the passenger...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>96</td>\n",
       "      <td>[-0.003155810758471489, -0.03044584020972252, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>of its passengers wanted to get off there, nor...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>97</td>\n",
       "      <td>[-0.0028652320615947247, -0.018465157598257065...</td>\n",
       "      <td>None</td>\n",
       "      <td>passengers waiting to be picked  up. At all ot...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>98</td>\n",
       "      <td>[-0.019418155774474144, -0.0311089139431715, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>right is the instantaneous reward at continuou...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>99</td>\n",
       "      <td>[0.0155481593683362, -0.018920114263892174, -0...</td>\n",
       "      <td>None</td>\n",
       "      <td>waiting times--is not something that would nor...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ids                                         embeddings metadatas  \\\n",
       "0      0  [0.022032495588064194, -0.07901791483163834, 0...      None   \n",
       "1      1  [0.027059582993388176, -0.06932812184095383, -...      None   \n",
       "2     10  [-0.012819928117096424, -0.0496579147875309, -...      None   \n",
       "3    100  [-0.012207344, -0.028850332, -0.054086257, -0....      None   \n",
       "4    101  [-0.008514339, -0.042759273, -0.043888994, -0....      None   \n",
       "..   ...                                                ...       ...   \n",
       "101   95  [-0.0011812966549769044, -0.052746474742889404...      None   \n",
       "102   96  [-0.003155810758471489, -0.03044584020972252, ...      None   \n",
       "103   97  [-0.0028652320615947247, -0.018465157598257065...      None   \n",
       "104   98  [-0.019418155774474144, -0.0311089139431715, -...      None   \n",
       "105   99  [0.0155481593683362, -0.018920114263892174, -0...      None   \n",
       "\n",
       "                                             documents  uris  data  \n",
       "0    Experiment. 1. \\nPerform Data Pre-processing o...  None  None  \n",
       "1    tasks for cleaning the data and making it suit...  None  None  \n",
       "2    economics (e.g., predicting growth), business ...  None  None  \n",
       "3    For function approximation, a non linear neura...  None  None  \n",
       "4    decision was required. Exactly o ne of these u...  None  None  \n",
       "..                                                 ...   ...   ...  \n",
       "101  average system time  is how long the passenger...  None  None  \n",
       "102  of its passengers wanted to get off there, nor...  None  None  \n",
       "103  passengers waiting to be picked  up. At all ot...  None  None  \n",
       "104  right is the instantaneous reward at continuou...  None  None  \n",
       "105  waiting times--is not something that would nor...  None  None  \n",
       "\n",
       "[106 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(db.peek(11097))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(db.peek(10)).iloc[0][\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve fact from the vector database\n",
    "\n",
    "def get_relevant_passages(query, db, n_results):\n",
    "    passages = db.query(query_texts=[query], n_results=n_results)['documents'][0]\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_response(response):\n",
    "    # Initialize an empty string to accumulate text\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Check if the response has a 'parts' attribute and iterate over it if present\n",
    "    if hasattr(response, 'parts'):\n",
    "        for part in response.parts:\n",
    "            if hasattr(part, 'text'):  # Ensure the part has a 'text' attribute\n",
    "                extracted_text += part.text + \"\\n\"  # Append the text from each part\n",
    "    \n",
    "    # Alternatively, if the structure is deeper (e.g., candidates with content parts):\n",
    "    elif hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
    "        for candidate in response.candidates:\n",
    "            for part in candidate.content.parts:\n",
    "                if hasattr(part, 'text'):\n",
    "                    extracted_text += part.text + \"\\n\"\n",
    "    \n",
    "    return extracted_text.strip()  # Return the combined text, stripping any trailing newline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_prompt(ques, knowledge, chats):\n",
    "    text = knowledge.replace(\"'\",\"\").replace('\"','') #even i dont know why i did this\n",
    "\n",
    "\n",
    "    # Choose from the given prompts which suits you the BEST\n",
    "    # If you want you may create your own prompt by referring to the following options\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # prompt = f\"\"\"question: {ques}.\\n\n",
    "    # information base or knowledge base: {text}\\n\n",
    "    # Answer the question in 1250-2500 words strictly based on the information base or knowledge base given, \\n\n",
    "    # If the question is asking for a code then also explain the algorithm, if the question is asking for career guidance then provide complete career guidance and links for various courses, if difference is asked differentiate with minimum 7 points in tabular format and one example\n",
    "    # if the information is not sufficient then give output as \"Info not good to answer\"\n",
    "    # \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    prompt = f\"\"\"question: {ques}.\\n\n",
    "    information base or knowledge base: {text}\\n\n",
    "    Answer the question strictly based from knowledge base by filtering the required information from knowledge base\\n\n",
    "    Generate a sophisticated and neat answer such that it could be written in exam\\n\n",
    "    If the knowledge base does not have data related to the question reply with \"Out of Syllabus\"\n",
    "    If the question is asking for a code then also explain the algorithm,\\n \n",
    "    if the question is asking for career guidance then provide complete career guidance and links for various courses, \\n\n",
    "    if difference is asked differentiate with minimum 7 points in tabular format and one example\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # prompt = f\"\"\"question: {ques}.\\n\n",
    "    # chat histoty: {chats}\\n\n",
    "    # information base or knowledge base: {text}\\n\n",
    "    # Work like a answer generarting bot for exam who generates answer for given {ques} based on the {text} provided. Generate output in such a way that it should be written in examination.\n",
    "    # check whether {ques} is a question or an instruction, if its an instruction give an affirmative response \\n\n",
    "    # ELSE\n",
    "    # Answer the question STRICTLY based from {text} by filtering the required information from {text}\\n\n",
    "    # if the information is not sufficient then use keywords from {text} related to {ques} and answer the queestion and at the end of answer mention \"[from other sources as well]\" BUT try to give answers from only and only{text}\\n\n",
    "    # If the question is asking for a code then also explain the algorithm,\\n \n",
    "    # if the question is asking for career guidance then provide complete career guidance and links for various courses, \\n\n",
    "    # if difference is asked differentiate with minimum 7 points in tabular format and one example\\n\n",
    "    # Also DO NOT forget to consider the previous qquestions and the answers you gave for it :{chats}\\n\n",
    "    # If the question refers about past questions or answers, generate the answer considering {chats} and {text}\n",
    "    # MOST IMPORTANTLY: answer the questions from {text} in such a way to write in exam\\n\n",
    "    # NOTE: try to answwer {ques} from {text} as much as possible, and bigger answer is prefered\n",
    "    # \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # prompt = f\"\"\"Given the question '{ques}', the previous chat history '{chats}', and a knowledge base '{text}':\n",
    "    # 1. Identify if '{ques}' is a question or an instruction. For instructions, provide an affirmative response.\n",
    "    # 2. If it's a question, answer it based on the information in '{text}'. Generate the answer which is suitable for an exam setting.\n",
    "    # 3. If '{ques}' requires information beyond '{text}', don't answer the question just reply \"Out of Syllabus\"\n",
    "    # 4. For coding questions, include an algorithm explanation. For career guidance inquiries, offer comprehensive advice and resource links. If asked to differentiate, provide at least 7 comparison points in a table with an example.\n",
    "    # 5. Incorporate relevant details from the chat history '{chats}' when answering, especially if the question refers to previous discussions.\n",
    "    # \"\"\" \n",
    "    \n",
    "    gen_config = GenerationConfig(temperature=0.1)\n",
    "    answer_text = model.generate_content(prompt,generation_config=gen_config)\n",
    "    answer = extract_text_from_response(answer_text)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:yellow\">Question:-</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \" How to implement Support Vector Machine \"\n",
    "passages = get_relevant_passages(ques, db, n_results=25) #i have kept the n_results more because i wanted more info to be included in my answwer\n",
    "txt = \"\"\n",
    "for passage in passages:\n",
    "    txt += passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(passages):\n",
    "    content = \"\"\n",
    "    for passage in passages:\n",
    "        content += passage + \"\\n\"\n",
    "    return content\n",
    "\n",
    "cont = list_to_string(passages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2. Objectives: From this experiment, the student will be able to   ● Familiar with support vector machine.  ● Understand and implement classification algorithms in machine l earning.  ● Analyse the large data using SVM.     3. Outcomes: The learner will be able to     ● Understand and design Support Vect or Machines for classificatio n of linear data.     4. Software Required :  Weka/ Java / Python/ Matlab / R-Tool etc.  5. Theory:    A Support Vector Machine (SVM) is a supervised machine learning  algorithm that can be  employed for both classification and regression purposes. SVMs are more commonly used  in classification problems. SVMs are based on the idea of findi ng a hyper plane that best  divides a dataset into two classes, as shown in the image below .  .   Support Vectors     Support vectors are the data points nearest to the hyperplane, the points of a data set that, if  removed, would alter the position of the dividing hyperplane. B ecause of this, they can be 2. Objectives: From this experiment, the student will be able to   ● Familiar with support vector machine.  ● Understand and implement classification algorithms in machine l earning.  ● Analyse the large data using SVM.     3. Outcomes: The learner will be able to     ● Understand and design Support Vect or Machines for classificatio n of linear data.     4. Software Required :  Weka/ Java / Python/ Matlab / R-Tool etc.  5. Theory:    A Support Vector Machine (SVM) is a supervised machine learning  algorithm that can be  employed for both classification and regression purposes. SVMs are more commonly used  in classification problems. SVMs are based on the idea of findi ng a hyper plane that best  divides a dataset into two classes, as shown in the image below .  .   Support Vectors     Support vectors are the data points nearest to the hyperplane, the points of a data set that, if  removed, would alter the position of the dividing hyperplane. B ecause of this, they can be based on what additionally they learnt after performing the exp eriment)    15.  Conclusion:  (Students\\xa0should\\xa0write\\xa0proper\\xa0conclusion\\xa0(in\\xa0the\\xa0form\\xa0of\\xa0discus sion)on\\xa0 their\\xa0own)   Classification is a supervised  learning problem. Through this e xperiment the need  for classification algorithm was recognized and understood, and  we implemented  a navie bayes classification algorithm which uses a probabilist ic approach.     16. Viva Questions:  ● What is supervised learning?  ● What are various classification algorithms?  ● What is prior probability of a class?  References:    https://www.datacamp.com/tutori al/naive-bayes-scikit-learn   https://www.javatpoint.com/machine‐learning‐naive‐bayes‐classif ier\\xa0     Machine Learning   Experiment No.: 7  Implement Support Vector Machine for  binary classification         Experiment No. 7  1. Aim:  To Implement Support Vector Machine for binary classification.      2. Objectives: From this experiment, the student will be able to based on what additionally they learnt after performing the exp eriment)    15.  Conclusion:  (Students\\xa0should\\xa0write\\xa0proper\\xa0conclusion\\xa0(in\\xa0the\\xa0form\\xa0of\\xa0discus sion)on\\xa0 their\\xa0own)   Classification is a supervised  learning problem. Through this e xperiment the need  for classification algorithm was recognized and understood, and  we implemented  a navie bayes classification algorithm which uses a probabilist ic approach.     16. Viva Questions:  ● What is supervised learning?  ● What are various classification algorithms?  ● What is prior probability of a class?  References:    https://www.datacamp.com/tutori al/naive-bayes-scikit-learn   https://www.javatpoint.com/machine‐learning‐naive‐bayes‐classif ier\\xa0     Machine Learning   Experiment No.: 7  Implement Support Vector Machine for  binary classification         Experiment No. 7  1. Aim:  To Implement Support Vector Machine for binary classification.      2. Objectives: From this experiment, the student will be able to others. Although the optimal policy for this problem is unknown , and the state of the art is  difficult to pin down because details of commercial dispatching  strategies are proprietary,  these learned dispatchers appeared to perform very well.     References:   1. http://incompleteideas.net/book/ebook/node107.html others. Although the optimal policy for this problem is unknown , and the state of the art is  difficult to pin down because details of commercial dispatching  strategies are proprietary,  these learned dispatchers appeared to perform very well.     References:   1. http://incompleteideas.net/book/ebook/node107.html strategy is useful for the features which have numeric data suc h as age, salary, year, etc. Here,  we will use this approach.  Splitting the Dataset into the Training set and Test set In machine learning data preprocessing, we divide our dataset i nto a training set and test set.  This is one of the crucial steps of data preprocessing as by do ing this, we can enhance the  performance of our machine learning model.Suppose, if we have g iven training to our machine  learning model by a dataset and we test it by a completely diff erent dataset. Then, it will create  difficulties for our model to understand the correlations betwe en the models.  If we train our model very well and its training accuracy is al so very high, but we provide a  new dataset to it, then it will decrease the performance. So we  always try to make a machine  learning model which performs well with the training set and al so with the test dataset. Here,  we can define these datasets as: strategy is useful for the features which have numeric data suc h as age, salary, year, etc. Here,  we will use this approach.  Splitting the Dataset into the Training set and Test set In machine learning data preprocessing, we divide our dataset i nto a training set and test set.  This is one of the crucial steps of data preprocessing as by do ing this, we can enhance the  performance of our machine learning model.Suppose, if we have g iven training to our machine  learning model by a dataset and we test it by a completely diff erent dataset. Then, it will create  difficulties for our model to understand the correlations betwe en the models.  If we train our model very well and its training accuracy is al so very high, but we provide a  new dataset to it, then it will decrease the performance. So we  always try to make a machine  learning model which performs well with the training set and al so with the test dataset. Here,  we can define these datasets as: measure of how much our weights need to be updated to attain mi nimum or ideally 0  error. In case you have more than one feature, you need to calc ulate the partial  derivative for each weight b0, b1 … bn where n is the number of  features.  (2)  3. Next we update the values of b0 and b1:  (3)  4. We repeat this process until our loss function is a very small value or ideally reaches 0  (meaning no errors and 100% accuracy). The number of times we r epeat this learning  process is known as iterations or epochs.   Steps:  1. Read and visualize the database  2. Divide the data to training set and test set (80:20)  3. Perform data normalization 4. Compute the method to make predictions using equation (1)  5. Evaluate the method to train the model using equations (2) a nd (3). Obtain b0  and b1  6. Train the model using training data  7. Make the predictions  8. Calculate accuracy    Output Analysis: (Students should write output analysis based on the output. Spe cify each measure of how much our weights need to be updated to attain mi nimum or ideally 0  error. In case you have more than one feature, you need to calc ulate the partial  derivative for each weight b0, b1 … bn where n is the number of  features.  (2)  3. Next we update the values of b0 and b1:  (3)  4. We repeat this process until our loss function is a very small value or ideally reaches 0  (meaning no errors and 100% accuracy). The number of times we r epeat this learning  process is known as iterations or epochs.   Steps:  1. Read and visualize the database  2. Divide the data to training set and test set (80:20)  3. Perform data normalization 4. Compute the method to make predictions using equation (1)  5. Evaluate the method to train the model using equations (2) a nd (3). Obtain b0  and b1  6. Train the model using training data  7. Make the predictions  8. Calculate accuracy    Output Analysis: (Students should write output analysis based on the output. Spe cify each Our goal is to find statistically significant values of the par ameters α and β that minimize the  difference between Y and Yₑ. If we are able to determine the op timum values of these two  parameters, then we will have the line of best fit that we can use to predict the values of Y,  given the value of X. How do we estimate α and β? We can use a method called ordinary least  squares.  Ordinary Least Squares    Green lines show the difference between actual values Y and estimate values Yₑ  The objective of the least squares method is to find values of α and β that minimize the sum of  the squared difference between Y and Yₑ. We will not go through  the derivation here, but using  calculus we can show that the values of the unknown parameters are as follows:    where X ̄ is the mean of X values and Ȳ is the mean of Y values.  If you are familiar with statistics, you may recognize β as sim ply   Cov(X, Y) / Var(X).  Implementation Steps: Our goal is to find statistically significant values of the par ameters α and β that minimize the  difference between Y and Yₑ. If we are able to determine the op timum values of these two  parameters, then we will have the line of best fit that we can use to predict the values of Y,  given the value of X. How do we estimate α and β? We can use a method called ordinary least  squares.  Ordinary Least Squares    Green lines show the difference between actual values Y and estimate values Yₑ  The objective of the least squares method is to find values of α and β that minimize the sum of  the squared difference between Y and Yₑ. We will not go through  the derivation here, but using  calculus we can show that the values of the unknown parameters are as follows:    where X ̄ is the mean of X values and Ȳ is the mean of Y values.  If you are familiar with statistics, you may recognize β as sim ply   Cov(X, Y) / Var(X).  Implementation Steps: our predicted value and the actual value. We will be using the L2 Loss Function to calculate  the error. Theoretically you can use any function to calculate the error. This function can be  broken down as:  1. Let the actual value be yᵢ. Let the value predicted using our m odel be denoted as  ȳᵢ. Find the difference between the actual and predicted value.   2. Square this difference.  3. Find the sum across all the values in training data.    Now that we have the error, we need to update the values of our  parameters to minimize this  error. This is where the “learning” actually happens, since our  model is updating itself based  on its previous output to obtain a more accurate output in the next step. Hence with each  iteration our model becomes mor e and more accurate. We will be using the Gradient Descent  Algorithm  to estimate our parameters. Another commonly used algorithm is  the Maximum  Likelihood Estimation.  The Gradient Descent Algorithm our predicted value and the actual value. We will be using the L2 Loss Function to calculate  the error. Theoretically you can use any function to calculate the error. This function can be  broken down as:  1. Let the actual value be yᵢ. Let the value predicted using our m odel be denoted as  ȳᵢ. Find the difference between the actual and predicted value.   2. Square this difference.  3. Find the sum across all the values in training data.    Now that we have the error, we need to update the values of our  parameters to minimize this  error. This is where the “learning” actually happens, since our  model is updating itself based  on its previous output to obtain a more accurate output in the next step. Hence with each  iteration our model becomes mor e and more accurate. We will be using the Gradient Descent  Algorithm  to estimate our parameters. Another commonly used algorithm is  the Maximum  Likelihood Estimation.  The Gradient Descent Algorithm The distance between the hyperplane and the nearest data point from either set is known as the  margin. The goal is to choose a hyperplane with the greatest po ssible margin between the  hyperplane and any point within the training set, giving a grea ter chance of new data being  classified correctly.    Pros & Cons of Support Vector Machines   Pros   ● Accuracy  ● Works well on smaller cleaner datasets  ● It can be more efficient because it uses a subset of training p oints  Cons   ● Isn’t suited to larger datasets as the training time with SVMs can be high  ● Less effective on noisier datas ets with overlapping classes  6. Procedure/Program:  The operation of the SVM algor ithm is based on finding the hype rplane that gives the  largest minimum distance to the training examples. Twice, this distance receives the  important name of margin  within SVM’s theory. Therefore, the optimal separating  hyperplane maximizes  the margin of the training data. The distance between the hyperplane and the nearest data point from either set is known as the  margin. The goal is to choose a hyperplane with the greatest po ssible margin between the  hyperplane and any point within the training set, giving a grea ter chance of new data being  classified correctly.    Pros & Cons of Support Vector Machines   Pros   ● Accuracy  ● Works well on smaller cleaner datasets  ● It can be more efficient because it uses a subset of training p oints  Cons   ● Isn’t suited to larger datasets as the training time with SVMs can be high  ● Less effective on noisier datas ets with overlapping classes  6. Procedure/Program:  The operation of the SVM algor ithm is based on finding the hype rplane that gives the  largest minimum distance to the training examples. Twice, this distance receives the  important name of margin  within SVM’s theory. Therefore, the optimal separating  hyperplane maximizes  the margin of the training data. 10. Under Kernel choose Polykernel  11. Click Close  12. Set c = 1  13. Set Filter Type to Normalize training data  14. Click OK  15. Click Start  \\xa0   8. Results:    The operation of the SVM algorithm is based on finding the hype rplane that gives the largest  minimum distance to the training examples. Twice, this distance  receives the important name  of margin  within SVM’s theory. Therefore, the optimal separating hyperpl ane maximizes  the  margin of the training data.    9. Conclusion:  The different classification algorithms of data mining were stu died and one among them named  support vector machine algorithm was implemented. The need for classification algorithm was  recognized and understood.   10. Viva Questions:   ● What is hyperplane?  ● What are the different  kernel functions?  ● What is the role of ma ximum margin in SVM?  11. References:  ●  Tom M.Mitchell “Machine Learning” McGraw Hill  \\xa0    Machine Learning  Experiment No. : 8  Implement K-means clustering 10. Under Kernel choose Polykernel  11. Click Close  12. Set c = 1  13. Set Filter Type to Normalize training data  14. Click OK  15. Click Start  \\xa0   8. Results:    The operation of the SVM algorithm is based on finding the hype rplane that gives the largest  minimum distance to the training examples. Twice, this distance  receives the important name  of margin  within SVM’s theory. Therefore, the optimal separating hyperpl ane maximizes  the  margin of the training data.    9. Conclusion:  The different classification algorithms of data mining were stu died and one among them named  support vector machine algorithm was implemented. The need for classification algorithm was  recognized and understood.   10. Viva Questions:   ● What is hyperplane?  ● What are the different  kernel functions?  ● What is the role of ma ximum margin in SVM?  11. References:  ●  Tom M.Mitchell “Machine Learning” McGraw Hill  \\xa0    Machine Learning  Experiment No. : 8  Implement K-means clustering Cov(X, Y) / Var(X).  Implementation Steps:    1. Generate data with nonzero mean and standard deviation (X). Als o, create random  data by multiplying a constant and adding residual which is an actual data (Y).  2. Calculate the mean of X and Y  Mean_X = X1+X2+…X_N/ N  Mean_Y = Y1+Y2+…Y_N/ N  3. Calculate α and β using       4. Compute the predicted output y  Yₑ = α + β X  5. Plot regression against actual data     Output Analysis: (Students should write output analysis based on the output. Spe cify each  output explicitly with output analysis)       Additional Learning: (Students should write additional learning on their own based o n what  additionally they learnt after performing the experiment)     \\uf0b7 Regression analysis allows you to understand the strength of re lationships between  variables.  \\uf0b7 Regression analysis tells you what predictors in a model are st atistically significant and  which are not. Cov(X, Y) / Var(X).  Implementation Steps:    1. Generate data with nonzero mean and standard deviation (X). Als o, create random  data by multiplying a constant and adding residual which is an actual data (Y).  2. Calculate the mean of X and Y  Mean_X = X1+X2+…X_N/ N  Mean_Y = Y1+Y2+…Y_N/ N  3. Calculate α and β using       4. Compute the predicted output y  Yₑ = α + β X  5. Plot regression against actual data     Output Analysis: (Students should write output analysis based on the output. Spe cify each  output explicitly with output analysis)       Additional Learning: (Students should write additional learning on their own based o n what  additionally they learnt after performing the experiment)     \\uf0b7 Regression analysis allows you to understand the strength of re lationships between  variables.  \\uf0b7 Regression analysis tells you what predictors in a model are st atistically significant and  which are not. tasks for cleaning the data and making it suitable for a machin e learning model which also  increases the accuracy and efficiency of a machine learning mod el. It involves below steps:  \\uf0b7 Getting the dataset  \\uf0b7 Importing libraries  \\uf0b7 Importing datasets  \\uf0b7 Finding Missing Data  \\uf0b7 Splitting dataset into training and test set  \\uf0b7 Feature scaling  Get the Dataset  The collected data for a particular problem in a proper format is known as the dataset .  Importing Libraries  In order to perform data preprocessing using Python, we need to  import some predefined  Python libraries.  Importing the Datasets  Save the dataset with .csv format and then import the data with  read_csv() function of pandas  library.  Handling Missing data: C alculating the mean we will calculate the mean of that column o r  row which contains any missing value and will put it on the pla ce of missing value. This  strategy is useful for the features which have numeric data suc h as age, salary, year, etc. Here, tasks for cleaning the data and making it suitable for a machin e learning model which also  increases the accuracy and efficiency of a machine learning mod el. It involves below steps:  \\uf0b7 Getting the dataset  \\uf0b7 Importing libraries  \\uf0b7 Importing datasets  \\uf0b7 Finding Missing Data  \\uf0b7 Splitting dataset into training and test set  \\uf0b7 Feature scaling  Get the Dataset  The collected data for a particular problem in a proper format is known as the dataset .  Importing Libraries  In order to perform data preprocessing using Python, we need to  import some predefined  Python libraries.  Importing the Datasets  Save the dataset with .csv format and then import the data with  read_csv() function of pandas  library.  Handling Missing data: C alculating the mean we will calculate the mean of that column o r  row which contains any missing value and will put it on the pla ce of missing value. This  strategy is useful for the features which have numeric data suc h as age, salary, year, etc. Here, economics (e.g., predicting growth), business (e.g., predicting  product sales, employee  performance), social science (e.g., predicting political leanin gs from gender or race), healthcare  (e.g., predicting blood pressure  levels from weight, disease on set from biological factors), and  more.  The basic idea is that if we can fit a linear regression model to observed data, we can then use  the model to predict any future values. There are two kinds of variables in a l inear regression  model:  \\uf0b7 The input or predictor variable is the variable(s) that help pr edict the value of the output  variable. It is commonly referred to as X.  \\uf0b7 The output variable is the variable that we want to predict. It  is commonly referred to  as Y.  To estimate Y using linear regression, we assume the equation:  Yₑ = α + β X  where Yₑ is the estimated or predicted value of Y based on our linear equation.  Our goal is to find statistically significant values of the par ameters α and β that minimize the economics (e.g., predicting growth), business (e.g., predicting  product sales, employee  performance), social science (e.g., predicting political leanin gs from gender or race), healthcare  (e.g., predicting blood pressure  levels from weight, disease on set from biological factors), and  more.  The basic idea is that if we can fit a linear regression model to observed data, we can then use  the model to predict any future values. There are two kinds of variables in a l inear regression  model:  \\uf0b7 The input or predictor variable is the variable(s) that help pr edict the value of the output  variable. It is commonly referred to as X.  \\uf0b7 The output variable is the variable that we want to predict. It  is commonly referred to  as Y.  To estimate Y using linear regression, we assume the equation:  Yₑ = α + β X  where Yₑ is the estimated or predicted value of Y based on our linear equation.  Our goal is to find statistically significant values of the par ameters α and β that minimize the Experiment No.6  9. Aim: Implement Naive Bayes classification algorithm.     10. Objectives:  ● Analyse the data, identify the problem and choose relevant algo rithms to apply.  ● Identify the classification application in machine learning.  Outcomes:   ● Students will be able to understand and implement classificatio n algorithms.    11. Software Required : R Studio /Python    12.  Theory:            Naïve Bayes algorithm is a supervised learning algorith m, which is  based on Bayes theorem and used for solving classification prob lems. Naïve Bayes  Classifier is one of the simple and most effective Classificati on algorithms which helps in  building the fast machine learning models that can make quick p redictions. It is a  probabilistic classifier, which means it predicts on the basis of the probability of an object.  Some popular examples of Naïve Bayes Algorithm are spam filtrat ion, Sentimental  analysis, and classifying articles. '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont = cont.replace('Module :1 Introduction to Big Data and Hadoop',' ') #this is subjective for this data\n",
    "cont = cont.replace('Lecture 1: Introduction to Big Data3Lecture 2: Hadoop Ecosystem 58  Lecture 3: Introduction to spark3Lecture 4: Hadoop vs Spark58',' ') #this is subjective for this data\n",
    "cont = cont.replace('\\n',' ')  #this is subjective for this data\n",
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = make_prompt(ques, cont, chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nclass Node:\\n    def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n\\n\\nclass LinkedList:\\n    def __init__(self):\\n        self.head = None\\n\\n    def insert_at_beginning(self, data):\\n        new_node = Node(data)\\n        new_node.next = self.head\\n        self.head = new_node\\n\\n    def insert_at_end(self, data):\\n        new_node = Node(data)\\n        if self.head is None:\\n            self.head = new_node\\n        else:\\n            current_node = self.head\\n            while current_node.next is not None:\\n                current_node = current_node.next\\n            current_node.next = new_node\\n\\n    def insert_at_index(self, index, data):\\n        if index == 0:\\n            self.insert_at_beginning(data)\\n        else:\\n            new_node = Node(data)\\n            current_node = self.head\\n            for i in range(index - 1):\\n                current_node = current_node.next\\n            new_node.next = current_node.next\\n            current_node.next = new_node\\n\\n    def delete_at_beginning(self):\\n        if self.head is not None:\\n            self.head = self.head.next\\n\\n    def delete_at_end(self):\\n        if self.head is not None:\\n            if self.head.next is None:\\n                self.head = None\\n            else:\\n                current_node = self.head\\n                while current_node.next.next is not None:\\n                    current_node = current_node.next\\n                current_node.next = None\\n\\n    def delete_at_index(self, index):\\n        if index == 0:\\n            self.delete_at_beginning()\\n        else:\\n            current_node = self.head\\n            for i in range(index - 1):\\n                current_node = current_node.next\\n            current_node.next = current_node.next.next\\n\\n    def print_list(self):\\n        current_node = self.head\\n        while current_node is not None:\\n            print(current_node.data, end=\" \")\\n            current_node = current_node.next\\n\\n\\nif __name__ == \"__main__\":\\n    ll = LinkedList()\\n    ll.insert_at_beginning(10)\\n    ll.insert_at_end(20)\\n    ll.insert_at_index(1, 15)\\n    ll.print_list()\\n    print()\\n    ll.delete_at_beginning()\\n    ll.delete_at_end()\\n    ll.delete_at_index(0)\\n    ll.print_list()\\n```'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a new Q&A pair\n",
    "def chat_history(ques, answer, chats):\n",
    "    next_id = len(chats) + 1\n",
    "    chats[next_id] = {\"question\": ques, \"answer\": answer}\n",
    "if len(chats):\n",
    "    if (chats[len(chats)]['question']!=ques or chats[len(chats)]['question']!=ques):\n",
    "        chat_history(ques=ques, answer=answer, chats=chats)\n",
    "    else:\n",
    "        print(\"same\")\n",
    "else:\n",
    "    chat_history(ques=ques, answer=answer, chats=chats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:yellow\">Answer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**How to implement Support Vector Machine**\n",
       "\n",
       "**1. Introduction**\n",
       "\n",
       "A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification and regression tasks. SVMs are particularly well-suited for classification problems, and they are often used in applications such as image recognition, text classification, and spam filtering.\n",
       "\n",
       "**2. How SVMs work**\n",
       "\n",
       "SVMs work by finding a hyperplane that separates the data into two classes. The hyperplane is chosen so that it maximizes the margin, which is the distance between the hyperplane and the closest data points from each class.\n",
       "\n",
       "**3. Implementing an SVM**\n",
       "\n",
       "To implement an SVM, you can use a variety of software libraries, such as scikit-learn in Python or LIBSVM in C++. The following steps outline the general process of implementing an SVM:\n",
       "\n",
       "1. Load the data into your chosen software library.\n",
       "2. Preprocess the data by scaling the features and normalizing the data.\n",
       "3. Choose a kernel function. The kernel function determines the shape of the hyperplane.\n",
       "4. Train the SVM model using the training data.\n",
       "5. Evaluate the model's performance on the test data.\n",
       "\n",
       "**4. Example**\n",
       "\n",
       "The following Python code shows how to implement an SVM using scikit-learn:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.svm import SVC\n",
       "\n",
       "# Load the data\n",
       "data = pd.read_csv('data.csv')\n",
       "\n",
       "# Preprocess the data\n",
       "data['feature1'] = data['feature1'].astype(float)\n",
       "data['feature2'] = data['feature2'].astype(float)\n",
       "data = (data - data.min()) / (data.max() - data.min())\n",
       "\n",
       "# Choose a kernel function\n",
       "kernel = 'linear'\n",
       "\n",
       "# Train the SVM model\n",
       "model = SVC(kernel=kernel)\n",
       "model.fit(data[['feature1', 'feature2']], data['label'])\n",
       "\n",
       "# Evaluate the model's performance\n",
       "score = model.score(data[['feature1', 'feature2']], data['label'])\n",
       "print('The accuracy of the model is:', score)\n",
       "```\n",
       "\n",
       "**5. Conclusion**\n",
       "\n",
       "SVMs are a powerful machine learning algorithm that can be used for a variety of classification tasks. They are relatively easy to implement, and they can achieve high accuracy on many different types of data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ' write a code to implement linkedlist in python ', 'answer': '1. **What is the difference between a Queue and a Stack?**\\n\\nA queue is a First-In-First-Out (FIFO) data structure, while a stack is a Last-In-First-Out (LIFO) data structure. This means that in a queue, the first element added is the first element removed, while in a stack, the last element added is the first element removed.\\n\\n2. **What are the advantages and disadvantages of using a queue?**\\n\\nAdvantages of using a queue:\\n\\n* Easy to implement\\n* Efficient for FIFO operations\\n* Can be used to implement other data structures, such as stacks and priority queues\\n\\nDisadvantages of using a queue:\\n\\n* Not as efficient for LIFO operations as a stack\\n* Can be difficult to implement in a circular buffer\\n\\n3. **What are the advantages and disadvantages of using a stack?**\\n\\nAdvantages of using a stack:\\n\\n* Easy to implement\\n* Efficient for LIFO operations\\n* Can be used to implement other data structures, such as queues and recursion\\n\\nDisadvantages of using a stack:\\n\\n* Not as efficient for FIFO operations as a queue\\n* Can be difficult to implement in a circular buffer\\n\\n4. **When should you use a queue and when should you use a stack?**\\n\\nYou should use a queue when you need to implement a FIFO data structure. For example, you could use a queue to implement a line of people waiting for service. You should use a stack when you need to implement a LIFO data structure. For example, you could use a stack to implement a stack of function calls.\\n\\n5. **Can you provide a table that compares queues and stacks?**\\n\\n| Feature | Queue | Stack |\\n|---|---|---|\\n| Data structure | FIFO | LIFO |\\n| Efficiency for FIFO operations | Efficient | Not as efficient |\\n| Efficiency for LIFO operations | Not as efficient | Efficient |\\n| Implementation | Easy | Easy |\\n| Circular buffer implementation | Difficult | Difficult |\\n| Uses | Line of people waiting for service | Stack of function calls |\\n\\n[from other sources as well]'}\n",
      "{'question': ' write a code to implement Linked list in python ', 'answer': '```python\\nclass Node:\\n    def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n\\n\\nclass LinkedList:\\n    def __init__(self):\\n        self.head = None\\n\\n    def insert_at_beginning(self, data):\\n        new_node = Node(data)\\n        new_node.next = self.head\\n        self.head = new_node\\n\\n    def insert_at_end(self, data):\\n        new_node = Node(data)\\n        if self.head is None:\\n            self.head = new_node\\n        else:\\n            current_node = self.head\\n            while current_node.next is not None:\\n                current_node = current_node.next\\n            current_node.next = new_node\\n\\n    def insert_at_index(self, index, data):\\n        if index == 0:\\n            self.insert_at_beginning(data)\\n        else:\\n            new_node = Node(data)\\n            current_node = self.head\\n            for i in range(index - 1):\\n                current_node = current_node.next\\n            new_node.next = current_node.next\\n            current_node.next = new_node\\n\\n    def delete_at_beginning(self):\\n        if self.head is not None:\\n            self.head = self.head.next\\n\\n    def delete_at_end(self):\\n        if self.head is not None:\\n            if self.head.next is None:\\n                self.head = None\\n            else:\\n                current_node = self.head\\n                while current_node.next.next is not None:\\n                    current_node = current_node.next\\n                current_node.next = None\\n\\n    def delete_at_index(self, index):\\n        if index == 0:\\n            self.delete_at_beginning()\\n        else:\\n            current_node = self.head\\n            for i in range(index - 1):\\n                current_node = current_node.next\\n            current_node.next = current_node.next.next\\n\\n    def print_list(self):\\n        current_node = self.head\\n        while current_node is not None:\\n            print(current_node.data, end=\" \")\\n            current_node = current_node.next\\n\\n\\nif __name__ == \"__main__\":\\n    ll = LinkedList()\\n    ll.insert_at_beginning(10)\\n    ll.insert_at_end(20)\\n    ll.insert_at_index(1, 15)\\n    ll.print_list()\\n    print()\\n    ll.delete_at_beginning()\\n    ll.delete_at_end()\\n    ll.delete_at_index(0)\\n    ll.print_list()\\n```'}\n",
      "Length of chats is: 2\n"
     ]
    }
   ],
   "source": [
    "if len(chats)>1:\n",
    "    print(chats[len(chats)-1])\n",
    "print(chats[len(chats)])\n",
    "print(f\"Length of chats is: {len(chats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **What is the difference between a Queue and a Stack?**\n",
       "\n",
       "A queue is a First-In-First-Out (FIFO) data structure, while a stack is a Last-In-First-Out (LIFO) data structure. This means that in a queue, the first element added is the first element removed, while in a stack, the last element added is the first element removed.\n",
       "\n",
       "2. **What are the advantages and disadvantages of using a queue?**\n",
       "\n",
       "Advantages of using a queue:\n",
       "\n",
       "* Easy to implement\n",
       "* Efficient for FIFO operations\n",
       "* Can be used to implement other data structures, such as stacks and priority queues\n",
       "\n",
       "Disadvantages of using a queue:\n",
       "\n",
       "* Not as efficient for LIFO operations as a stack\n",
       "* Can be difficult to implement in a circular buffer\n",
       "\n",
       "3. **What are the advantages and disadvantages of using a stack?**\n",
       "\n",
       "Advantages of using a stack:\n",
       "\n",
       "* Easy to implement\n",
       "* Efficient for LIFO operations\n",
       "* Can be used to implement other data structures, such as queues and recursion\n",
       "\n",
       "Disadvantages of using a stack:\n",
       "\n",
       "* Not as efficient for FIFO operations as a queue\n",
       "* Can be difficult to implement in a circular buffer\n",
       "\n",
       "4. **When should you use a queue and when should you use a stack?**\n",
       "\n",
       "You should use a queue when you need to implement a FIFO data structure. For example, you could use a queue to implement a line of people waiting for service. You should use a stack when you need to implement a LIFO data structure. For example, you could use a stack to implement a stack of function calls.\n",
       "\n",
       "5. **Can you provide a table that compares queues and stacks?**\n",
       "\n",
       "| Feature | Queue | Stack |\n",
       "|---|---|---|\n",
       "| Data structure | FIFO | LIFO |\n",
       "| Efficiency for FIFO operations | Efficient | Not as efficient |\n",
       "| Efficiency for LIFO operations | Not as efficient | Efficient |\n",
       "| Implementation | Easy | Easy |\n",
       "| Circular buffer implementation | Difficult | Difficult |\n",
       "| Uses | Line of people waiting for service | Stack of function calls |\n",
       "\n",
       "[from other sources as well]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chats[1]['answer'])\n",
    "# n = len(chats)\n",
    "# id = 1\n",
    "# while (id<=n):\n",
    "#     Markdown(chats[id]['question'])\n",
    "#     Markdown(chats[id]['answer'])\n",
    "#     id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Danger</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will erase your chat history after a limit. Created this part because sometimes my Gemini LLM generates an error-500: Internal server error, it causes due to overloading of the model.\n",
    "# chats = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### /Users/mihiresh/Mihiresh/Hackathon/gdsc2/api_testing/rag2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
